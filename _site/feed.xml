<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-12-10T10:45:39-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Arun Kumar</title><subtitle>project associate&lt;br/&gt;rbccps.org</subtitle><author><name>Arun Kumar</name></author><entry><title type="html">Build a 2D Robotic Car!</title><link href="http://localhost:4000/posts/2017/04/robotic-car/" rel="alternate" type="text/html" title="Build a 2D Robotic Car!" /><published>2017-04-03T14:56:19-07:00</published><updated>2017-04-03T14:56:19-07:00</updated><id>http://localhost:4000/posts/2017/04/build-2d-robotic-car</id><content type="html" xml:base="http://localhost:4000/posts/2017/04/robotic-car/">&lt;p&gt;In the last blog post I explained what is a particle filter and how we can build one using pygame and python.In this post, I will walk you through the steps to build a 2D robotic car and get it running using PD control.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;I am building visual demonstration of the problems given in Udacity’s Course on Artificial Intelligence for Robotics.You can find the implementation code &lt;a href=&quot;https://github.com/ioarun/ai-for-robotics-udacity/blob/master/visual-implementation/racetrack-control.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt; Bicycle Model &lt;/h2&gt;

&lt;p&gt;Our robotic car has two fixed rear wheels and two front steereable wheels.The vehicle motion can be explained by splitting the vehicle vertically so that we get something called a bicycle model as shown below.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/robotic-car/bicycle-model1.png&quot; alt=&quot;Bicycle Model&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;Bicycle model - local coordinates&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;This model is useful in understanding the vehicle motion especially when taking a turn.Let’s say our car starts to take a right turn. Assuming the steering angle to be &lt;code class=&quot;highlighter-rouge&quot;&gt;alpha&lt;/code&gt;, the radius of the circle that the rear wheel makes while taking the turn to be &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt;, and the distance between the front and rear wheel be &lt;code class=&quot;highlighter-rouge&quot;&gt;L&lt;/code&gt;.Then the radius &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt; can be clearly stated as follows.&lt;/p&gt;

&lt;html&gt;
&lt;head&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;
&lt;head&gt;
&lt;body&gt;
 $$R = {L \over \tan \alpha}$$
&lt;/body&gt;
&lt;html&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/robotic-car/bicycle-model2.png&quot; alt=&quot;Bicycle Model&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;Bicycle model - global coordinates&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

We are interested in finding what would be the position and orientation of our car after it rotates by an angle `beta`.Let initial orientation with respect to a global `x = 0` be `theta` and the initial position be `(x, y)`.Then from the above figure, it is clear that the center of the circle which the car makes while steering, can be written as -

&lt;html&gt;
&lt;head&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;
&lt;head&gt;
&lt;body&gt;
 $$Cx = {x - Rsin \theta}$$
 $$Cy = {y + Rcos \theta}$$
&lt;/body&gt;
&lt;html&gt;

Since we are using Pygame to build this car, the second equation above changes a bit. As the origin of a pygame screen would be on the top left corner and will increase as we go right or downwards, going up would mean decrease in the value.Hence, the second equation becomes

&lt;html&gt;
&lt;head&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;
&lt;head&gt;
&lt;body&gt;
 $$Cy = {y - Rcos \theta}$$
&lt;/body&gt;
&lt;html&gt;

Now that we have got the center of the circle, we can find the new position `(x', y')` and orientation `theta`' as follows -

&lt;html&gt;
&lt;head&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;
&lt;head&gt;
&lt;body&gt;
 $$x' = {Cx + Rsin (\theta + \beta)}$$
 $$y' = {Cy - Rcos (\theta + \beta)}$$
 $${\theta} ' = {(\theta + \beta) mod 2 \pi}$$

&lt;/body&gt;
&lt;html&gt;

Again, we have to change the second equation according to the pygame coordinate system.

&lt;html&gt;
&lt;head&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML&quot;&gt;
&lt;/script&gt;
&lt;head&gt;
&lt;body&gt;
 $$y' = {Cy + Rcos (\theta + \beta)}$$
&lt;/body&gt;
&lt;html&gt;

Equipped with the knowledge of steering angle `alpha`, and current orientation `theta`, we can successfully find new position and orientation of the car at any instant.Our car is a massless car so the physics dynamics are not applicable in our case.The motion is explained by simple trigonometry.

Now let's build the controller for our car.The controller that I have used is a simple PD(Proportional Derivative) controller.The controller decides the steering angle for the car at any instant.And it does so based on some inputs from the car or the environment in which the car is.In the lectures, the instructor has used Cross-Track Error(CTE) as an input to the controller.So, I have used the same approach.

The CTE is the deviation from the reference line on the car's track.I have designed a race track which has rectangular mid-section (300x400) and two semi-circular sections(radius = 200) on the left and right sides of the rectangle.In this case, the reference line changes in each section.So there are total 4 cases.


&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cte&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;cte&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;orientation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;orientation&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;250&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;250&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;cte&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;250&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;550&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;orientation&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;orientation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; \
				&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;orientation&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;orientation&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;cte&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
				    &lt;span class=&quot;n&quot;&gt;cte&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;550&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;cte&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cte&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;



&lt;h2&gt; PD Controller &lt;/h2&gt;

The PD Controller adjusts the steering angle by doing two very intuitive things.If the car is very far from the reference line, the steering angle should be very high.Also, the CTE will be very high.This means, that the steering angle is proportional to the CTE.Now, the only thing the car is basing its value of steering angle is the CTE.It might happen that the value of steering angle is more than required and it overshoots.How would the car know it is nearing the reference line? One way to do is by adding a derivative term in the controller.This will keep the track of how fast the CTE is changing.If there is not much change, the derivative value will be less and it would mean that the car is near the reference line. The value of steering angle then would be small.


&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;steering_angle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;crosstrack_error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff_crosstrack_error&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;


&lt;h2&gt; Car body &lt;/h2&gt;

The car body is a 80px by 60px yellow trianlge with four 20px by 6px wheels.In the last blog post I had used a car sprite for demonstrating the particle filter localization.It was pretty easy because image rotation method is available in pygame.But rectangle rotation is not available.So I decided to write my own method for rectangle rotation.The method draw_rect() takes four arguments center, corners, rotation_angle and color.The center is center of the rectangle, corners is a list of the position of 4 corners before rotation, rotation_angle is the angle by which the rectangle will rotate.


&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;draw_rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corners&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rotation_angle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;c_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;c_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;delta_angle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rotation_angle&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;rotated_corners&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corners&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;angle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;atan2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;angle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta_angle&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;angle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;angle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;rotated_corners&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		
		&lt;span class=&quot;c&quot;&gt;# draw rectangular polygon --&amp;gt; car body&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pygame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;draw&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;polygon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;screen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;\
		&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rotated_corners&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rotated_corners&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rotated_corners&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;\
		&lt;span class=&quot;n&quot;&gt;rotated_corners&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;


&lt;p&gt; The method iterates through all the four corners and calculates the length of each from the center.And using atan2, gets the angle between center and the corner before rotation.Then the rotation angle delta_angle is added to this.The new position is calculated using simple trigonotmetry equations as shown above.&lt;/p&gt;

&lt;p&gt;I was specially interested in implementing this method because I wanted to see steering wheels in action.In wheel motion, there are three angles involved.Let's take just one steering wheel.First we must know the new center of the wheel after rotation.We get this by rotating the current center of the wheel by orientation angle.Then using this newly found center, we draw four corners of the wheel (aligned in +x direction).We call draw_rect() using newly found center and the corners.We find the angle between each corner and the new center of the wheel.Next we rotate the corners by steering angle to get final positon of each corner of the wheel.&lt;/p&gt;

You can find the implementation code and the racetrack image that I have used &lt;a href=&quot;https://github.com/ioarun/ai-for-robotics-udacity/blob/master/visual-implementation/racetrack-control.py&quot;&gt;here&lt;/a&gt;.I hope you found this post helpful.Until next time!

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/robotic-car/racetrack-control.gif&quot; alt=&quot;Robotic Car&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;Robotic car in motion! Observe how PD control is acting on steering wheels.&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;


&lt;/html&gt;&lt;/head&gt;&lt;/head&gt;&lt;/html&gt;&lt;/html&gt;&lt;/head&gt;&lt;/head&gt;&lt;/html&gt;&lt;/html&gt;&lt;/head&gt;&lt;/head&gt;&lt;/html&gt;&lt;/html&gt;&lt;/head&gt;&lt;/head&gt;&lt;/html&gt;&lt;/html&gt;&lt;/head&gt;&lt;/head&gt;&lt;/html&gt;</content><author><name>arun</name></author><category term="robotics" /><category term="artificial-intelligence" /><summary type="html">In the last blog post I explained what is a particle filter and how we can build one using pygame and python.In this post, I will walk you through the steps to build a 2D robotic car and get it running using PD control.</summary></entry><entry><title type="html">Robot Localization using Particle Filter</title><link href="http://localhost:4000/posts/2017/03/particle-filter/" rel="alternate" type="text/html" title="Robot Localization using Particle Filter" /><published>2017-03-07T13:56:19-08:00</published><updated>2017-03-07T13:56:19-08:00</updated><id>http://localhost:4000/posts/2017/03/particle-filter</id><content type="html" xml:base="http://localhost:4000/posts/2017/03/particle-filter/">&lt;p&gt;Robot world is exciting! For people completely unaware of what goes inside the robots and how they manage to do what they do, it seems almost magical.In this post, with the help of an implementation, I will try to scratch the surface of one very important part of robotics called robot localization.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;what-is-localization&quot;&gt;What is localization?&lt;/h2&gt;

&lt;p&gt;Imagine that you are blind folded and somehow all you know about the surrounding is as far as you can stretch out your hands to touch the surrounding obstacles.This would give you an approximation of the distance of an obstacle from you in the surrounding but not your orientation in global coordinate system(surrounding).This information is not sufficient to know your exact location in the surrounding.&lt;/p&gt;

&lt;p&gt;Trying to find the near exact location of the robot in a given space using noisy sensor data is called as &lt;em&gt;localization&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Take another example of a moving car.The GPS might show the car coordinates pretty well but when it comes to relying on just GPS values for autonomous driving, it is not that pretty.The error may go as far as 10 meters which can be fatal to the car.&lt;/p&gt;

&lt;p&gt;Localization makes sure that we reduce this error by as much as possible from the given sensor information and dynamics of the car.&lt;/p&gt;

&lt;h2 id=&quot;particle-filter&quot;&gt;Particle Filter&lt;/h2&gt;

&lt;p&gt;Consider the first example where you had to examine the surrounding by your hands.Suppose there are N of you and are randomly spread out in the surrounding and each of you have a distance vector which contains the distances from each of the obstacles.Since there are 1000 of yous, many of them would be in the vicinity of you near the same obstacle.Ofcourse the orientation may vary.Currently all the human beings are uniformally distributed.Until the next step which is called resampling.In resampling process only the humans near you would remain and others will disappear.You are said to be somewhat localized.&lt;/p&gt;

&lt;p&gt;Formally, a particle is a discrete guess of where a robot may be located.Also, regularly, these particles are resampled from the distribution so that the particles consistent with the measurements &lt;em&gt;survive&lt;/em&gt;.After successfull localization, the particles are collected in a region of high probability of the robot.&lt;/p&gt;

&lt;h2 id=&quot;localize-a-car&quot;&gt;Localize a Car!&lt;/h2&gt;

&lt;p&gt;Having taken the CS373 course on Udacity on AI for robotics, I had an urge to build my own robotic self-driving car.I am in the process to build it in simulation.This post is one part of it.I tried to build a 2D demo of robot-car localization using Python and Pygame.You can find the source code &lt;a href=&quot;https://github.com/ioarun/pygame-robotics/blob/master/particle-filter/particle-filter-2.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-robot-class&quot;&gt;The robot class&lt;/h3&gt;

&lt;p&gt;This class contains robot specific methods like&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;set()&lt;/code&gt; -  to set x, y and orientation&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;set_noise()&lt;/code&gt; - to set forward motion noise, turn noise and bearing noise&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;move()&lt;/code&gt; - to move robot&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sense()&lt;/code&gt; - to sense the bearing angle (angle between heading of robot and landmark location)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Instantiation of the robot class creates an object with random location and orientation.Let’s call this object &lt;code class=&quot;highlighter-rouge&quot;&gt;car&lt;/code&gt;. Then we will create &lt;code class=&quot;highlighter-rouge&quot;&gt;particles&lt;/code&gt;. Make an empty list called &lt;code class=&quot;highlighter-rouge&quot;&gt;particles&lt;/code&gt; and append it with the robot objects called &lt;code class=&quot;highlighter-rouge&quot;&gt;particle&lt;/code&gt;.I created 1000 such particles.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/particle-filter/initial_condition.png&quot; alt=&quot;Initial state of particles&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;1000 particles distributed uniformly in the space&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;The figure above is a screen shot of my pygame screen.The blue circles are the landmarks.The car sprite is located in the center of the screen in the initial state and particles (green dots) are uniformly distributed on the screen.&lt;/p&gt;

&lt;p&gt;At this current frame, several things are happening.The car &lt;em&gt;senses&lt;/em&gt; the landmarks and generates a &lt;em&gt;measurements&lt;/em&gt; vector with as many values as their are landmarks.These measurements are &lt;em&gt;bearing angles&lt;/em&gt; or the angle between car heading and the landmark location.Ofcourse our sensors are noisy.So a bearing noise which is a guassian around mean 0 is added to these measurements.&lt;/p&gt;

&lt;h3 id=&quot;weights&quot;&gt;Weights&lt;/h3&gt;

&lt;p&gt;Since the particles are uniformly distributed, some would be closer to the car and some would be far.So the particles nearby car are good estimate of the car’s location.&lt;em&gt;Weights&lt;/em&gt; are the measure of how important the particles are.The larger the weight, the more important it is.
Next we allow the particles to survive at random, but the probability of survival will be proportional to the weights. That is, a particle with a larger weight will survive at a higher proportion than a particle with a small weight. This means that after resampling, which is randomly drawing new particles from the old ones with
replacement in proportion to the importance weight, the particles with a higher importance weight will live on, while the smaller ones will die out.With replacement means high probability particle would be drawn multiple number of times.So at any time there will be total 1000 particles.&lt;/p&gt;

&lt;p&gt;There is one &lt;em&gt;measurement_prob()&lt;/em&gt; method provided in the course that Sebastian Thrun, the course instructor wrote which finds these weights proportional to likelihood of the measurement.The argument consists of single &lt;em&gt;measurements&lt;/em&gt; vector.Based on the difference in the actual measurement by the car and the predicted measurement by the particle into consideration and the Gaussian the method returns the likelihood of the particle.This is repeated for all the 1000 particles.&lt;/p&gt;

&lt;h3 id=&quot;resampling&quot;&gt;Resampling&lt;/h3&gt;

&lt;p&gt;In the resampling process, highly weighted particles are selected with a high probability than the ones with low weights.These weights are &lt;em&gt;normalized&lt;/em&gt; weights meaning, they are divided by the sum total of all the weights.Suppose we have 6 particles - p1, p2, p3, p4, p5, p6 and p3 and p6 have higher weights than the other four particles.We will run a resampling cycle 6 times.So, let’s say the first time p6 was chosen.Second time again p6 was chosen, next p1 was chosen followed by p3, p3 and p5.So the new particles list will look like - p1, p3, p3, p5, p6, p6.Eventually after many iterations, the list may look like p6, p6, p6, p6, p6, p6.And we will see 6 of these particles approximately at the car location and will say that our car/robot is localized.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/particle-filter/next_condition.png&quot; alt=&quot;Localized car&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;Small green dot is actually 1000 particles on top of each other localizing the car.&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;&lt;em&gt;Resampling wheel&lt;/em&gt; is one way to implement resampling.It is very efficient 10 lines of code.We represent all the particles and their weights on a big wheel.Each slice on the wheel is proportional to the corresponding weights.Now we chose any index from 0 to 999 at random.Let’s say we chose &lt;code class=&quot;highlighter-rouge&quot;&gt;W6&lt;/code&gt; randomly.Create a variable &lt;code class=&quot;highlighter-rouge&quot;&gt;beta&lt;/code&gt; and assign a value &lt;code class=&quot;highlighter-rouge&quot;&gt;2*max(weights)&lt;/code&gt; to it.Now, we follow following steps until the value of &lt;code class=&quot;highlighter-rouge&quot;&gt;beta&lt;/code&gt; is less than the weight at running index.Subtract &lt;code class=&quot;highlighter-rouge&quot;&gt;weight[index]&lt;/code&gt; from &lt;code class=&quot;highlighter-rouge&quot;&gt;beta&lt;/code&gt; and increment &lt;code class=&quot;highlighter-rouge&quot;&gt;index&lt;/code&gt;.As soon as &lt;code class=&quot;highlighter-rouge&quot;&gt;beta&lt;/code&gt; drops below the running weight, loop terminates and the particle associated with running index is appended to the new particle list.&lt;/p&gt;

&lt;p&gt;Above steps are repeated till all the 1000 particles are not appended to the new particles list.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/particle-filter/resampling-wheel.jpeg&quot; alt=&quot;Resampling wheel&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;[Source: Udacity]Resampling wheel&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next steps!&lt;/h2&gt;

&lt;p&gt;That was just one frame or instant.In the next frame, the car moves and so do the particles.These particles are the ones from previous step of resampling.The process of localization starts again and repeats as explained above.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/particle-filter/particle-filter.gif&quot; alt=&quot;Particle Filter Localization&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;Particle Filter Localization&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;That’s it for now guys.I had fun implementing the particle filter.I am aiming to build a basic implementation of self driving car in pygame.More posts on the same soon.Do follow for updates!&lt;/p&gt;</content><author><name>arun</name></author><category term="robotics" /><category term="artificial-intelligence" /><summary type="html">Robot world is exciting! For people completely unaware of what goes inside the robots and how they manage to do what they do, it seems almost magical.In this post, with the help of an implementation, I will try to scratch the surface of one very important part of robotics called robot localization.</summary></entry><entry><title type="html">Convolutional Neural Network</title><link href="http://localhost:4000/posts/2017/01/cnn/" rel="alternate" type="text/html" title="Convolutional Neural Network" /><published>2017-01-30T13:56:19-08:00</published><updated>2017-01-30T13:56:19-08:00</updated><id>http://localhost:4000/posts/2017/01/cnn</id><content type="html" xml:base="http://localhost:4000/posts/2017/01/cnn/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Convolutional Neural Networks or ConvNets or CNNs are biologically inspired varients of Multilayer Perceptrons(MLPs).They are probably the biggest reasons why AI agents are able to play ATARI games, are creating master piece artwork and cars have learnt to drive by themselves.Not only this, they are also being used in Natural language processing and text classification.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;In this post, I will try to explain the basic architecture of the CNN.Much of the following content is derived from &lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;CS231n&lt;/a&gt;.It’s a great course and helps build the intuition of the working of CNN.Let me start by why is there a need for CNN and why it is becoming so popular.&lt;/p&gt;

&lt;h2 id=&quot;regular-neural-nets-do-not-scale&quot;&gt;Regular Neural Nets do not scale&lt;/h2&gt;

&lt;p&gt;Suppose we have a &lt;code class=&quot;highlighter-rouge&quot;&gt;200x200x3&lt;/code&gt; size image(&lt;code class=&quot;highlighter-rouge&quot;&gt;3&lt;/code&gt; is the number of channels - R, G, B) and we input it to our first hidden layer in the regular neural nets.This would lead to a total of &lt;code class=&quot;highlighter-rouge&quot;&gt;200x200x3 = 120,000&lt;/code&gt; weights.And that is just one layer.As we consider more number of layers, the parameters would add up quickly.Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/cnn/comparing-regular-and-cnn.jpeg&quot; alt=&quot;Regular Neural Net&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt; A regular 3-layer Neural Network&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;h2 id=&quot;3d-volumes-of-neurons&quot;&gt;3D Volumes of Neurons&lt;/h2&gt;

&lt;p&gt;In particular, unlike a regular Neural Network, the layers of a ConvNet have neurons arranged in &lt;code class=&quot;highlighter-rouge&quot;&gt;3&lt;/code&gt; dimensions: width, height, depth. (Note that the word depth here refers to the third dimension of an activation volume, not to the depth of a full Neural Network, which can refer to the total number of layers in a network.) For example, the input images in CIFAR-10 are an input volume of activations, and the volume has dimensions &lt;code class=&quot;highlighter-rouge&quot;&gt;32x32x3&lt;/code&gt; (width, height, depth respectively). The neurons in a layer will only be connected to a small region of the layer before it, instead of all of the neurons in a fully-connected manner. This model is actually found in animal visual cortex.Individual cortical neurons respond to stimuli in a restricted region of space known as the receptive field. The receptive fields of different neurons partially overlap such that they tile the visual field. The response of an individual neuron to stimuli within its receptive field can be approximated mathematically by a convolution operation.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/cnn/comparing-regular-and-cnn2.jpeg&quot; alt=&quot;Convolutional Neural Net&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;Every layer of a ConvNet transforms the 3D input volume to a 3D output volume of neuron activations. In this example, the red input layer holds the image, so its width and height would be the dimensions of the image, and the depth would be `3` (Red, Green, Blue channels).&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;h2 id=&quot;what-is-convolution&quot;&gt;What is convolution?&lt;/h2&gt;

&lt;p&gt;Convolution can be thought of as a sliding window function applied to a matrix.This sliding window is called as &lt;em&gt;filter&lt;/em&gt;, &lt;em&gt;kernel&lt;/em&gt; or &lt;em&gt;feature extractor&lt;/em&gt;.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/cnn/Convolution_schematic.gif&quot; alt=&quot;convolution&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;Convolution operation - elementwise multiplication and then adding the results&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;h1 id=&quot;cnn-architecture&quot;&gt;CNN Architecture&lt;/h1&gt;

&lt;p&gt;Let’s take each component one by one.&lt;/p&gt;

&lt;h2 id=&quot;input-layer&quot;&gt;Input layer&lt;/h2&gt;

&lt;p&gt;Unlike regular neural nets where we have one-dimensional input vector, here we have 3D image as the input.The three dimensions are &lt;code class=&quot;highlighter-rouge&quot;&gt;height&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;width&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;channels&lt;/code&gt; (&lt;code class=&quot;highlighter-rouge&quot;&gt;3&lt;/code&gt; if RGB image, &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt; if grayscale.)&lt;/p&gt;

&lt;h2 id=&quot;filter&quot;&gt;Filter&lt;/h2&gt;

&lt;p&gt;A filter is a matrix of weights which slides over the input image and does dot product with the image matrix just below it simultaneously.It then passes the result through a ReLU or tanH activation neuron thus outputting single element of &lt;em&gt;activation map&lt;/em&gt;.The dimensions of a filter depend on the input &lt;em&gt;thickness&lt;/em&gt;. If the input image/layer has &lt;code class=&quot;highlighter-rouge&quot;&gt;3&lt;/code&gt; channels/thickness, the filter dimension would be something like 5x5x3.&lt;/p&gt;

&lt;h2 id=&quot;convolution-layer&quot;&gt;Convolution layer&lt;/h2&gt;

&lt;p&gt;As we slide the filter over the input image, we get activations and all of them as a whole form one layer of activation map.The convolution layer is made up of these activation maps and each neuron on the activation map has a &lt;em&gt;local connectivity&lt;/em&gt; with a local region in the input volume.Imagine flashing a torch on a flat transparent surface in a dark room with a condition that the light(yellow in color) doesn’t scatter as it penetrates the depth.&lt;em&gt;The volume under this light would look like the part of the input volume that a convolution layer neuron is connected to at this particular moment&lt;/em&gt;.Sure, as we move our filter, the volume shifts accordingly but the dimensions and the &lt;em&gt;weights&lt;/em&gt; remain the same. Let’s calculate the total number of weights connected to each neuron in each activation map.For a &lt;code class=&quot;highlighter-rouge&quot;&gt;5x5x3&lt;/code&gt; filter, a input volume of &lt;code class=&quot;highlighter-rouge&quot;&gt;32x32x3&lt;/code&gt; will produce &lt;code class=&quot;highlighter-rouge&quot;&gt;5x5x3 = 75&lt;/code&gt; wights in one layer.&lt;/p&gt;

&lt;p&gt;These weights are shared among all the other neurons in one activation map.If the output layer has 6 activation maps of dimension &lt;code class=&quot;highlighter-rouge&quot;&gt;28x28&lt;/code&gt;, then &lt;code class=&quot;highlighter-rouge&quot;&gt;75&lt;/code&gt; weights will be shared by all the other &lt;code class=&quot;highlighter-rouge&quot;&gt;783&lt;/code&gt; neurons on one activation map.This is called as &lt;em&gt;parameter sharing&lt;/em&gt;.What about depth then?Are these weights also shared with each neuron accross the depth dimension.No, they are not.Infact there will be as many sets of weights as their are activation maps/filters.Imagine the torch light exampled I gave above.Now we have another torch that flashes a different color light say green.This torch would be just below the first one and covering the same area and volume.BUT, it’s a different color now.Similar is the case with activation maps in convolution layer.Each map has it’s own set of weights which are different than the maps above and below it but are shared among the neurons on the same map.The torch analogy might be a wrong example to state here but I hope this clears the picture a little.Following figure may make things clearer.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/cnn/neurons-in-conv.jpeg&quot; alt=&quot;Convolution layer&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;A 32x32x3 volume input connected to a neuron in convolutional layer&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;There is a simple math involved as to calculate the dimensions of output activation map depending on the the input and filter dimensions and quantity.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Output size = (N - F)/Stride + 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;where  &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt; = input spatial dimension (eg. a &lt;code class=&quot;highlighter-rouge&quot;&gt;32x32x3&lt;/code&gt; image would have &lt;code class=&quot;highlighter-rouge&quot;&gt;N = 32&lt;/code&gt;),
&lt;code class=&quot;highlighter-rouge&quot;&gt;F&lt;/code&gt; = filter dimension (eg. &lt;code class=&quot;highlighter-rouge&quot;&gt;3x3&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;5x5&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;11x11&lt;/code&gt; )&lt;/p&gt;

&lt;p&gt;The depth of the activation map or number of layers in the output(slices of activation maps) depend on the number of filters we are using.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Activation map depth = Number of filters used.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;But what are &lt;em&gt;strides&lt;/em&gt; ? Strides are the number of steps to move while sliding over input.
There is one more thing left which is padding.Let’s look at the significance of padding by an example.Suppose we have a 32x32 image and we are using a filter of size &lt;code class=&quot;highlighter-rouge&quot;&gt;5x5&lt;/code&gt;.Taking stride as &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt;, what will be the dimension of the output after one convolution?&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(N-F)/S + 1 = (32-5)/1 + 1 = 28
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Okay, now let’s use this output as an input to out next convolution.The result will be &lt;code class=&quot;highlighter-rouge&quot;&gt;24&lt;/code&gt;. So we see that the output shrinks gradually at the beginning.This was just an example.In real life scenario, it shrinks even faster.Padding helps us avoid this situation by keeping the output dimension either same or more.It is a process in which the input is padded with &lt;code class=&quot;highlighter-rouge&quot;&gt;0's&lt;/code&gt; on height and width sides.For an example,in the following image, a &lt;code class=&quot;highlighter-rouge&quot;&gt;7x7&lt;/code&gt; image has been padded with &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt; pixel border.What is the corresponding output dimension?&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/cnn/padding.png&quot; alt=&quot;Padding&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;Padding helps maintain the output dimension(spatially)&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;N = 9, F = 3, S = 1
(N-F)/S + 1 = (9-3)/1 + 1 = 7
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;That’s same as the input! Thus we see that we can control the shrinking of our output by padding.We can also modify about formula to add padding.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(N−F+2P)/S + 1
where P is padding(0,1,2,3...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Following formulae from &lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;CS231&lt;/a&gt; summarize the convolution layer very well.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Input (W2xH2xD2) ---convolution--&amp;gt; Output (W2xH2xD2)

where W2 = (W1-F+2P)/S + 1
H2 = (H1-F+2P)/S + 1
D2 = K

Total number of weights per filter = FxFxD1 
Total number of weights in a convolution layer = (FxFxD1)xK + K biases
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;pooling&quot;&gt;Pooling&lt;/h2&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/cnn/downsampling.jpeg&quot; alt=&quot;Downsampling&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride 2 into
output volume of size [112x112x64]&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;Pooling let’s us control the spatial size of the output layer and thus controlling the number of parameters.It is usually inserted between two convolution layers.It is done one each activation map independently and preserves the depth dimension.In &lt;em&gt;max pooling&lt;/em&gt; operation, a filter (eg. 2x2, stride 2) slides over one activation map and downsamples it to half of it’s previous dimensions.Following figure shows the max pooling operation.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/cnn/max-pooling.jpeg&quot; alt=&quot;max-pooling&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;A very common max pooling operation done over a slice of activation map.Max operation is performed over 4 numbers at a time when a filter of 2x2 makes stride of 2&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;h2 id=&quot;fully-connected-layer&quot;&gt;Fully connected layer&lt;/h2&gt;

&lt;p&gt;The neuron activations from the final convolution layers are flattened to produce a vector which acts as an input to a regular neural network called fully connected network.This network finally produces outputs like scores or probabilities associated with our original input volume.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/cnn/fc-cnn.jpeg&quot; alt=&quot;Fully connected layer&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;h1 id=&quot;putting-it-all-together&quot;&gt;Putting it all together&lt;/h1&gt;

&lt;p&gt;Though I have not covered all the details of a CNN network, above mentioned components are pretty much what is required for building a basic CNN network like handwritten digits classifier.In general, the full CNN architecture can be summarized by the following formula:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;INPUT -&amp;gt; [[CONV -&amp;gt; RELU]*N -&amp;gt; POOL?]*M -&amp;gt; [FC -&amp;gt; RELU]*K -&amp;gt; FC
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;where the &lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt; indicates repetition, and the &lt;code class=&quot;highlighter-rouge&quot;&gt;POOL?&lt;/code&gt; indicates an optional pooling layer.
Moreover, &lt;code class=&quot;highlighter-rouge&quot;&gt;N &amp;gt;= 0&lt;/code&gt; (and usually &lt;code class=&quot;highlighter-rouge&quot;&gt;N &amp;lt;= 3&lt;/code&gt; ), &lt;code class=&quot;highlighter-rouge&quot;&gt;M &amp;gt;= 0&lt;/code&gt; , &lt;code class=&quot;highlighter-rouge&quot;&gt;K &amp;gt;= 0&lt;/code&gt; (and usually &lt;code class=&quot;highlighter-rouge&quot;&gt;K &amp;lt; 3&lt;/code&gt; ).&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/cnn/cnn-title-image.jpeg&quot; alt=&quot;LeNet&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;LeNet-5[LeCun et al., 1998] had CONV-POOL-CONV-POOL-CONV-FC architecture&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;I hope this post was useful in some way.Please follow for more interesting updates!&lt;/p&gt;</content><author><name>arun</name></author><category term="cnn" /><category term="convolutional-neural-networks" /><category term="machine-learning" /><summary type="html">Convolutional Neural Networks or ConvNets or CNNs are biologically inspired varients of Multilayer Perceptrons(MLPs).They are probably the biggest reasons why AI agents are able to play ATARI games, are creating master piece artwork and cars have learnt to drive by themselves.Not only this, they are also being used in Natural language processing and text classification.</summary></entry><entry><title type="html">Understanding Hierarchical Temporal Memory</title><link href="http://localhost:4000/posts/2017/01/htm/" rel="alternate" type="text/html" title="Understanding Hierarchical Temporal Memory" /><published>2017-01-15T13:56:19-08:00</published><updated>2017-01-15T13:56:19-08:00</updated><id>http://localhost:4000/posts/2017/01/htm</id><content type="html" xml:base="http://localhost:4000/posts/2017/01/htm/">&lt;p&gt;Last year I did one project on Cognitive Healthcare which used Hierarchical Temporal Memory or HTM.Through this post, I have tried to put down my understanding of &lt;a href=&quot;http://numenta.org/&quot;&gt;Numenta&lt;/a&gt;’s HTM.
Before getting to it, it is important to understand the functioning of the neocortex to process sensory inputs from the physical world because HTM is inspired by the same.
&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;htm-is-inspired-by-neocortex&quot;&gt;HTM is inspired by neocortex&lt;/h2&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/htm/brain-five-lobes.jpg&quot; alt=&quot;neocortex&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;Human Neocortex&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;Broadly speaking, a brain is divided into four major lobes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the frontal lobe (associated with reward, attention, short-term memory tasks, planning, and motivation)&lt;/li&gt;
  &lt;li&gt;the parietal lobe (involved in integrating sensory information from the various senses, and in the manipulation of objects in determining spatial sense and navigation)&lt;/li&gt;
  &lt;li&gt;the occipital lobe (visual)&lt;/li&gt;
  &lt;li&gt;the temporal lobe (involved with the senses of smell and sound, the processing of semantics in both speech and vision, including the processing of complex stimuli like faces and scenes, and plays a key role in the formation of long-term memory)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In humans, the neocortex is the size of a dinner napkin 2.5 mm thick and it is folded to fit inside our skull. Older parts of the brain that are below the neocortex are involved in the basic functions of life like eating, sleeping, and getting emotional. But that is not where intelligence resides.  The neocortex is the part that gives you memory and stores all the lessons that you learned.&lt;/p&gt;

&lt;h2 id=&quot;structure-of-neocortex&quot;&gt;Structure of neocortex&lt;/h2&gt;

&lt;p&gt;The neocortex is generally said to have six layers. Five of the layers contain cells and one layer is mostly connections. The image below (from &lt;a href=&quot;https://en.wikipedia.org/wiki/Santiago_Ram%C3%B3n_y_Cajal&quot;&gt;Santiago Ramon y Cajal&lt;/a&gt;) shows a small slice of neocortex exposed using three different staining methods. The vertical axis spans the thickness of the neocortex, approximately 2mm. The left side of the image indicates the six layers. Layer 1, at the top, is the non-cellular level. The “WM” at the bottom indicates the beginning of the white matter, where axons from cells travel to other parts of the neocortex and other parts of the brain.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/htm/cajal_cortex.jpeg&quot; alt=&quot;cajal neocortex&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
&lt;/div&gt;
&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;Although Layer 2 and 3 look easily distinguished they are normally considered as one layer i.e, Layer 2/3.&lt;/li&gt;
  &lt;li&gt;Layer 4 is the most well defined in those neocortical regions which are closest to the sensory organs.&lt;/li&gt;
  &lt;li&gt;Layer 5 has the largest cells in the cortex.&lt;/li&gt;
  &lt;li&gt;Layer 6 projects different sub-cortical areas and sends feedback to the thalamus.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you cut out any portion of neocortex and observe at cellular level, every portion looks alike no matter from where you cut it out from – visual, auditory, or any other part of brain – all the same.
&lt;em&gt;This tells us that the brain deals with the sensory inputs in an almost identical manner&lt;/em&gt; . It uses the same sets of algorithms for every kind of information.
There is one more specialty about the structure of the neorocortex. It is observed that the neurons are aligned vertically in columns. And the neurons in the same column respond to the same type of input.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/htm/columnar.jpeg&quot; alt=&quot;columnar architecture&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
&lt;/div&gt;
&lt;/center&gt;

&lt;h2 id=&quot;mapping-the-neocortex-to-the-htm-model&quot;&gt;Mapping the Neocortex to the HTM Model&lt;/h2&gt;

&lt;p&gt;As there is a hierarchy of layers in neocortex, so is the case in HTM. The raw data is sensed by the lower levels and then the processed data is feed-forwarded to the next higher level in the hierarchy.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/htm/heirarchy.jpg&quot; alt=&quot;hierarchical view&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
&lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;An HTM can be composed of just one region, or several regions with data being fed through them. The region itself is comprised of interconnected cells that can be in one of three states:
*Active from feed-forward input.
*Active from lateral input (a prediction)
*Inactive.&lt;/p&gt;

&lt;p&gt;The cells are stacked on top of each other in vertical columns forming a three dimensional grid:&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2017/htm/prediction.jpg&quot; alt=&quot;prediction&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
&lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;The HTM cortical learning algorithm shows that a layer of cells organized in columns can be a high-capacity memory of variable order state transitions. Stated more simply, a layer of cells can learn a lot of sequences.
Columns of cells that share the same feed-forward response are the key mechanism for learning variable-order transitions.&lt;/p&gt;

&lt;h2 id=&quot;sparse-distributed-representations&quot;&gt;Sparse Distributed Representations&lt;/h2&gt;

&lt;p&gt;The human cortex has about 20 – 30 billion neurons. At any point of time, the active neurons are just 2% of that number. Thus, information in the brain is always represented by a small percentage of active neurons within a large population of neurons. This kind of encoding is called a sparse distributed representation (SDR). HTM also uses SDRs. The input to an HTM region is always a distributed representation, but it may not be sparse, so the first thing an HTM region does is convert its input into a sparse distributed representation.
&lt;em&gt;We can represent an input as a SDR by an array of 1’s and 0’s&lt;/em&gt; . Each bit in the array represents a neuron or cell.&lt;/p&gt;

&lt;h2 id=&quot;how-are-the-predictions-made&quot;&gt;How are the predictions made?&lt;/h2&gt;

&lt;p&gt;The cells are grouped in vertical columns, and all the cells in a column get the same feed-forward input (either directly from the data, or the output from a lower HTM region). The input, as mentioned previously, is converted to a sparse distributed representation, and then each column gets a unique subset of that input (usually overlapping with other columns, but never the exact same subset). Cells also receive lateral input from other columns of cells.
Which cells get activated within a column depends on a variety of factors, and the use of columns allows the HTM to create temporal contexts. This &lt;a href=&quot;http://numenta.com/assets/pdf/whitepapers/hierarchical-temporal-memory-cortical-learning-algorithm-0.2.1-en.pdf&quot;&gt;white&lt;/a&gt; paper explains:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;By selecting different active cells in each active column, we can represent the exact same input differently in different contexts. A specific example might help. Say every column has 4 cells and the representation of every input consists of 100 active columns. If only one cell per column is active at a time, we have 4^100 ways of representing the exact same input. The same input will always result in the same 100 columns being active, but in different contexts different cells in those columns will be active.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Which cells in a column become active depends on the current activation state of those cells:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;When a column becomes active, it looks at all the cells in the column. If one or more cells in the column are already in the predictive state, only those cells become active. If no cells in the column are in the predictive state, then all the cells become active. You can think of it this way, if an input pattern is expected then the system confirms that expectation by activating only the cells in the predictive state. If the input pattern is unexpected then the system activates all cells in the column as if to say “the input occurred unexpectedly so all possible interpretations are valid”.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Real neural networks contain inhibitory signals as well as excitatory ones. The same concept is applied in HTM regions as part of the lateral input that cells receive from their neighboring columns:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The columns with the strongest activation inhibit, or deactivate, the columns with weaker activation. (The inhibition occurs within a radius that can span from very local to the entire region.) The sparse representation of the input is encoded by which columns are active and which are inactive after inhibition. The inhibition function is defined to achieve a relatively constant percentage of columns to be active, even when the number of input bits that are active varies significantly.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Lateral input can inhibit as well as excite cells, and this is how HTM regions form predictions:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;When input patterns change over time, different sets of columns and cells become active in sequence. When a cell becomes active, it forms connections to a subset of the cells nearby that were active immediately prior. These connections can be formed quickly or slowly depending on the learning rate required by the application. Later, all a cell needs to do is to look at these connections for coincident activity. If the connections become active, the cell can expect that it might become active shortly and enters a predictive state. Thus the feed-forward activation of a set of cells will lead to the predictive activation of other sets of cells that typically follow. Think of this as the moment when you recognize a song and start predicting the next notes.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The influence one cell has on another is moderated by the strength of the a virtual synapse between, which can have a decimal value between 0 and 1.&lt;/p&gt;

&lt;h2 id=&quot;tools-to-implement-htm--nupic&quot;&gt;Tools to implement HTM : NuPIC&lt;/h2&gt;

&lt;p&gt;Numenta Platform for Intelligent Computing or &lt;a href=&quot;http://numenta.org/&quot;&gt;NuPIC&lt;/a&gt; is an open source project based on HTM.It learns the time-based patterns in data , predicts future values and detects anomalies.You can very easily follow the instructions given on their website and start building your own application around NuPIC.&lt;/p&gt;

&lt;p&gt;I hope you found this blog helpful.Cheers!&lt;/p&gt;</content><author><name>arun</name></author><category term="cnn" /><category term="convolutional-neural-networks" /><category term="machine-learning" /><summary type="html">Last year I did one project on Cognitive Healthcare which used Hierarchical Temporal Memory or HTM.Through this post, I have tried to put down my understanding of [Numenta](http://numenta.org/)'s HTM. Before getting to it, it is important to understand the functioning of the neocortex to process sensory inputs from the physical world because HTM is inspired by the same.</summary></entry><entry><title type="html">Implementing Physical Web using Eddystone</title><link href="http://localhost:4000/posts/2016/05/physical-web/" rel="alternate" type="text/html" title="Implementing Physical Web using Eddystone" /><published>2016-05-05T14:56:19-07:00</published><updated>2016-05-05T14:56:19-07:00</updated><id>http://localhost:4000/posts/2016/05/eddystone-beacon</id><content type="html" xml:base="http://localhost:4000/posts/2016/05/physical-web/">&lt;p&gt;Hi everyone! This blog post is about a project I did couple of months ago on Physical Web.I will not go deep into the literature and will dive straight into the implementation part.For more information on Physical Web follow this &lt;a href=&quot;https://google.github.io/physical-web/&quot;&gt;link&lt;/a&gt;.
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;Choosing hardware – Any BLE 4.0+ chip would do.In our project we have used Read Bear Lab’s
BLE Shield which incorporates Nrf8001 bluetooth low energy chip.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2016/physical-web/one.png&quot; alt=&quot;one&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;Red Bear Lab BLE Shield&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;h2 id=&quot;how-to-create-a-beacon&quot;&gt;How To Create a Beacon?&lt;/h2&gt;

&lt;p&gt;Beacons are tiny modules which broadcast radio signals continuously or after regular intervals of
time( in our case it is 100 ms).In this project we have used the above mentioned shield and Arduino
Mega board to &lt;em&gt;create&lt;/em&gt; a beacon.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2016/physical-web/two.jpg&quot; alt=&quot;two&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;Arduino Mega&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;h2 id=&quot;library-used--bleperipheral-by-sandeep-mistry&quot;&gt;Library Used : BLEPeripheral by Sandeep Mistry&lt;/h2&gt;

&lt;p&gt;This library has number of examples, one of which is Eddystone.h Including this header in our arduino code, we were able to create our arduino + BLE Sheild into a beacon. Ofcourse the practical implementation required us to rewrite basic implementation to suit our own use case.Starting point of this use case was how we broadcast a simple URL.
Before going further, note that Eddystone frame format supports three ( now four) frames – URL, UID, TLM.&lt;/p&gt;

&lt;h2 id=&quot;eddystone-url&quot;&gt;Eddystone URL:&lt;/h2&gt;
&lt;p&gt;Eddystone allows for 31 Bytes of data to be transmitted in one frame.So if your URL exceeds 31 Bytes, you can use google shortner to shorten the URL.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Every Eddystone frame has to have 16 bit Service UUID &lt;em&gt;0xFEAA&lt;/em&gt; which will be used by any scanner application to know that the incoming frame is an eddystone.&lt;/li&gt;
  &lt;li&gt;This then will be followed by 1 Byte of frame specific identifier i.e, &lt;em&gt;0x00&lt;/em&gt; for UID, &lt;em&gt;0x10&lt;/em&gt; for URL, &lt;em&gt;0x20&lt;/em&gt; for TLM.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;After this follows the actual data.We will be transmitting our URL in this slot.There is a rule by which we encode our URL into the frame.More can be found out &lt;a href=&quot;https://github.com/google/eddystone/tree/master/eddystone-url&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Basic code :&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// Import libraries (EddystonBeacon depends on SPI)
&lt;/span&gt;&lt;span class=&quot;cp&quot;&gt;#include &amp;lt;SPI.h&amp;gt;
#include &amp;lt;EddystoneBeacon.h&amp;gt;
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;//define pins (varies per shield/board)
// Adafruit Bluefruit LE     10, 2, 9
// Blend                      9, 8, UNUSED
// Blend  Micro               6, 7, 4
// RBL BLE Shield             9, 8, UNUSED
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;EddystoneBeacon&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eddystoneBeacon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;EddystoneBeacon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EDDYSTONE_BEACON_REQ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
	&lt;span class=&quot;n&quot;&gt;EDDYSTONE_BEACON_RDY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EDDYSTONE_BEACON_RST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;setup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;Serial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;begin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9600&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;eddystoneBeacon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;begin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;18&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;http://goo.gl/TTY3vU&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;Serial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Eddystone URL Beacon&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;eddystoneBeacon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;EddystoneBeacon eddystoneBeacon = EddystoneBeacon(…) makes an object of the class with three arguments which are the pins.The actual definition inside the library ( &lt;em&gt;EddystoneBeacon.cpp&lt;/em&gt; ) looks as follows:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;&lt;span class=&quot;n&quot;&gt;EddystoneBeacon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EddystoneBeacon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
	&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;BLEPeripheral&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;_bleService&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;feaa&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;_bleCharacteristic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;feab&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BLERead&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BLEBroadcast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MAX_SERVICE_DATA_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setAdvertisedServiceUuid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_bleService&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uuid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;());&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setConnectable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;addAttribute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_bleService&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;addAttribute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_bleCharacteristic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;There are couple of important things inside this definition.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Services&lt;/em&gt; and &lt;em&gt;Characteristics&lt;/em&gt;- A call to this constructor initializes our beacon with service and characteristics.Service is a 128 bit data which acts as an identity of the eddystone frame.Next comes the characteristics which can be considered the main elements that give certain properties to your BLE device.You can customize in any which way you want to. Just that at the scanning end you have to already know this data.It acts as a carrier of the information.We can similarly create characteristic for transmission and reception.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;SetConnectable&lt;/em&gt; is false because initially our beacon is in broadcast only mode&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;EddystoneBeacon.begin(...)&lt;/code&gt; will begin broadcasting the power and URL data.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-cpp&quot; data-lang=&quot;cpp&quot;&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;EddystoneBeacon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;begin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_power&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;power&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setURI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;BLEPeripheral&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;begin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_bleCharacteristic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;broadcast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;EddystoneBeacon.loop()&lt;/code&gt; will poll infinitely.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;use-case--smart-meter&quot;&gt;Use Case : Smart Meter&lt;/h2&gt;

&lt;p&gt;We created a setup which consists of an electronic valve( 230V) , one 5V relay, one Push Button , an LED and the beacon.
The idea was to simulate a scenario where a bike driver when in the vicinity of a petrol pump, will get notified on his/her smarphone about it.He/she can then approach the pump, tap on the notification and get redirected to a web-page/web-application which enabled him/her to interact with the petrol pump.Once he sees the webpage, he can pair his phone to the petrol pump( beacon ) and enter the volume of the fluid to be filled in his phone.On sending the command to fill, the beacon recieves it and turns the valve ON till the said amount is filled.
At last, this guy can disconnect from the beacon.&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2016/physical-web/three.png&quot; alt=&quot;three&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;Block Diagram&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;h2 id=&quot;web-bluetooth-api-and-the-web-application&quot;&gt;Web Bluetooth API and the Web Application&lt;/h2&gt;

&lt;p&gt;To use google’s web bluetooth API, we need to have chrome 49 and above.Since we will be viewing  our web application on android phone, one more constraint is that it should be Android 6+.
A web application was made on IBM Bluemix with route &lt;a href=&quot;https://thislinknolongerworks.mybluemix.net&quot;&gt;https://thislinknolongerworks.mybluemix.net&lt;/a&gt;.
The web application runs a node server at the back end.In the front end, what we see is html consisting of javascript.This is where we write our Web Bluetooth code.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Requesting the bluetooth adaptor&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;	  &lt;span class=&quot;nx&quot;&gt;bluetoothDevice&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;writeButton&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;querySelector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'#fillUp'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;navigator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;bluetooth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;requestDevice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;filters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[{&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;services&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'713d0000-503e-4c75-ba94-3148f18d941e'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Notice here &lt;em&gt;[{services: […]}]&lt;/em&gt; is the serivice number for BLE tx and rx.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Connect to the GATT Server.This call will return a server object that will be used to connect to BLE Service.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;	&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;bluetoothDevice&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;bluetoothDevice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;connectGATT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;server&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// Getting Service...&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;getPrimaryService&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'713d0000-503e-4c75-ba94-3148f18d941e'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;After this, the object returned will be used to seperately access the Tx and Rx Characteristics.We will add an eventlistner and handler function to the Tx Characteristic and will send the commands to our beacon using Rx Characteristic.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;	&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;service&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;bleService&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;//alert(&quot;primaryService passed!&quot;);&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// Getting Characteristic...&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;bleService&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;getCharacteristic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'713d0003-503e-4c75-ba94-3148f18d941e'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;characteristic&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;writeCharacteristic&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;characteristic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;bleService&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;getCharacteristic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'713d0002-503e-4c75-ba94-3148f18d941e'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;characteristic&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;myCharacteristic&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;characteristic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myCharacteristic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;startNotifications&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;myCharacteristic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;addEventListener&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'characteristicvaluechanged'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;handleNotifications&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;alert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Ready.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;alert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Handler Function&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;	&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;handleNotifications&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;event&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;event&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;textDecoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;TextDecoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;//alert(textDecoder.decode(value));&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;//document.getElementById(&quot;setVolume&quot;).value = textDecoder.decode(value);&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;setVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;parseInt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;textDecoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;setVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;setVal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;//DONUT CHART&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;donut&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;Morris&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Donut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'setVolume'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;colors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;#3c8dbc&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;#f56954&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Full%&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;setVal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Empty%&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;setVal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;hideHover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'auto'&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Writing data to beacon/Sending Commands&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;  	&lt;span class=&quot;nx&quot;&gt;writeButton&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;addEventListener&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'click'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;c1&quot;&gt;// Get the bytes for the text&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;getElementById&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;inputValue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;encoder&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;TextEncoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;utf-8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;encoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;writeCharacteristic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;writeValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;workflow-overview&quot;&gt;Workflow overview:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Beacon –broadcasts–&amp;gt; URL&lt;/li&gt;
  &lt;li&gt;Smartphone Scans for the URL.&lt;/li&gt;
  &lt;li&gt;User taps on it.&lt;/li&gt;
  &lt;li&gt;Opens a web application.&lt;/li&gt;
  &lt;li&gt;User pushes the button on beacon to enter it in read/write mode.&lt;/li&gt;
  &lt;li&gt;On web app, user touches toggle button to connect to beacon.&lt;/li&gt;
  &lt;li&gt;Pairs it with beacon.&lt;/li&gt;
  &lt;li&gt;Enters volume in the text field.Hits Fill Up.&lt;/li&gt;
  &lt;li&gt;Beacon is listening to the data.&lt;/li&gt;
  &lt;li&gt;Recieves volume related data.&lt;/li&gt;
  &lt;li&gt;Goes into a loop and sends integers 0 to 9 at a regular interval decided by the volume related data it just recieved.&lt;/li&gt;
  &lt;li&gt;Meanwhile, LED and electronic valve is ON till last integer 9 is sent back to the web application.&lt;/li&gt;
  &lt;li&gt;Disconnect.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Check out all the codes &lt;a href=&quot;https://github.com/ioarun/eddystoneBeacon&quot;&gt;here&lt;/a&gt; and if you liked the post do follow for future updates!&lt;/p&gt;</content><author><name>arun</name></author><category term="cnn" /><category term="convolutional-neural-networks" /><category term="machine-learning" /><summary type="html">Hi everyone! This blog post is about a project I did couple of months ago on Physical Web.I will not go deep into the literature and will dive straight into the implementation part.For more information on Physical Web follow this [link](https://google.github.io/physical-web/).</summary></entry><entry><title type="html">Galileo-Link - Connect THINGS to Social Network</title><link href="http://localhost:4000/posts/2016/01/galileo-link/" rel="alternate" type="text/html" title="Galileo-Link - Connect THINGS to Social Network" /><published>2016-01-01T13:56:19-08:00</published><updated>2016-01-01T13:56:19-08:00</updated><id>http://localhost:4000/posts/2016/01/galileo-link-post</id><content type="html" xml:base="http://localhost:4000/posts/2016/01/galileo-link/">&lt;p&gt;Hello people!The following project is a basic implementation of what can later give rise to a dedicated social network for sensors. Have you ever wondered how would it be if all your inanimate things can talk on social network like Facebook, twitter? Green-house updating it’s health on it’s timeline? This project is an effort to give social life to the “things”.
&lt;!--more--&gt;&lt;/p&gt;

&lt;!-- _includes/image.html --&gt;
&lt;center&gt;
&lt;div class=&quot;image-wrapper&quot;&gt;
    
        &lt;img align=&quot;middle&quot; src=&quot;http://localhost:4000/images/2016/galileo-link/2016-01-01-galileo-link.jpg&quot; alt=&quot;CIFAR-10 example images&quot; style=&quot;border: 2px solid black;&quot; /&gt;
    
    
        &lt;p align=&quot;center&quot; class=&quot;image-caption&quot; style=&quot;font-size:14px;&quot;&gt;Galileo-link posts room temperature and light values on it's timeline&lt;/p&gt;
    
&lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;I have implemented the project using Intel Galileo board as the sensor gateway, you can use any other board running Linux OS with python installed. I tried making my home talk. That is, posting sensors data installed on Intel Galileo kept in my room to the Facebook account by name “Galileo Link” wall. So all we need is access to Facebook Graph API and a python script.&lt;/p&gt;

&lt;p&gt;To access the Graph API, we need to create a Facebook App on &lt;a href=&quot;https://developers.facebook.com/apps&quot;&gt;facebook developers&lt;/a&gt; page.Now follow these steps:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Click &lt;code class=&quot;highlighter-rouge&quot;&gt;Add a New App&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Choose platform WWW&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Choose a new name for your app&lt;/code&gt; -&amp;gt; Click &lt;code class=&quot;highlighter-rouge&quot;&gt;Create New Facebook App ID&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Create a New App ID&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Choose Category&lt;/code&gt; -&amp;gt; Click &lt;code class=&quot;highlighter-rouge&quot;&gt;Create App ID&lt;/code&gt; again.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;2.Go back to Apps Dashboard -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Select the new app&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Settings&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Basic&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Enter Contact Email&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;3.Go to &lt;code class=&quot;highlighter-rouge&quot;&gt;Status &amp;amp; Review&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Do you want to make this app and all its live features available to the general public?&lt;/code&gt; -&amp;gt; Toggle the button to &lt;code class=&quot;highlighter-rouge&quot;&gt;Yes&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Make App Public?&lt;/code&gt; -&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Yes&lt;/code&gt;. This will enable others to see posts by your app in their timelines – otherwise, only you will see the wall posts by the app.&lt;/p&gt;

&lt;p&gt;4.Now – you should see a green dot next to app’s name, and the text &lt;code class=&quot;highlighter-rouge&quot;&gt;This app is public and available to all users&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;5.Make a note of the &lt;code class=&quot;highlighter-rouge&quot;&gt;App ID&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;App Secret&lt;/code&gt; (Click Show next to it; you will be asked to re-enter your Facebook password). Now that the app has been created, the next step is to get Facebook OAuth Token.&lt;/p&gt;

&lt;h2 id=&quot;steps-to-get-facebook-oauth-token-&quot;&gt;Steps to get Facebook OAuth Token :&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Go to &lt;a href=&quot;https://developers.facebook.com/tools/explorer/&quot;&gt;Graph API Explorer&lt;/a&gt;-&amp;gt; In the Application drop down -&amp;gt; Select the app created -&amp;gt; Click &lt;code class=&quot;highlighter-rouge&quot;&gt;Get Access Token&lt;/code&gt; -&amp;gt; In Permissions popup go to &lt;code class=&quot;highlighter-rouge&quot;&gt;Extended Permissions&lt;/code&gt; tab -&amp;gt; Select &lt;code class=&quot;highlighter-rouge&quot;&gt;manage_pages&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;publish_actions&lt;/code&gt;.These permissions will allow your app to publish posts acting as the page -&amp;gt; Click &lt;code class=&quot;highlighter-rouge&quot;&gt;Get Access Token&lt;/code&gt; -&amp;gt; You will see a message saying &lt;code class=&quot;highlighter-rouge&quot;&gt;{App} would like to post publicly to Facebook for you. Who do you want to share these posts with?&lt;/code&gt; -&amp;gt; I chose &lt;code class=&quot;highlighter-rouge&quot;&gt;Public for maximum visibility&lt;/code&gt; – as I wanted to post to a public page.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Make a note of the short-lived token shown in Graph API Explorer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Facebook has deprecated offline access, the next best thing is long-lived token which expires in 60 days. We will convert the short-lived access token noted above to a long-lived token. For that, fill in the values in the URL below and open it in a browser:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://graph.facebook.com/oauth/access_token?client_id{APP_ID}&amp;amp;client_secret{APP_SECRET}&amp;amp;grant_type=fb_exchange_token&amp;amp; fb_exchange_token={EXISTING_ACCESS_TOKEN}&quot;&gt;https://graph.facebook.com/oauth/access_token?client_id{APP_ID}&amp;amp;client_secret{APP_SECRET}&amp;amp;grant_type=fb_exchange_token&amp;amp; fb_exchange_token={EXISTING_ACCESS_TOKEN}&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Replace the values in {}.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;You should see &lt;code class=&quot;highlighter-rouge&quot;&gt;access_token={...}&amp;amp;expires={...}&lt;/code&gt;. This new &lt;code class=&quot;highlighter-rouge&quot;&gt;access_token&lt;/code&gt; is the long-lived token we will use in our Python script.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Long-lived token will also expire eventually, be prepared to perform this Step 3 again before that happens!&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We will use &lt;a href=&quot;https://github.com/pythonforfacebook/facebook-sdk&quot;&gt;Facebook Python SDK&lt;/a&gt; to access Facebook’s Graph API. You can install it using pip: &lt;code class=&quot;highlighter-rouge&quot;&gt;pip install facebook-sdk&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;facebook&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;facebook&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__path__&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The path to the facebook will be displayed.Go to the directory delete facebook packages and then re-install it again.Problem solved!&lt;/p&gt;

&lt;p&gt;Now the next important thing is the page ID which can be found in About tab of the Facebook account (usually it is a big number). Finally the code on Intel Galileo will post to Facebook wall of the Account.
Follow this &lt;a href=&quot;https://github.com/ioarun/galileo-link&quot;&gt;link&lt;/a&gt; to my github and find the python script for this project.
Here is the &lt;a href=&quot;https://www.facebook.com/profile.php?id=100009541012933&quot;&gt;link&lt;/a&gt; to my Galileo-Link!
Do follow for more interesting posts!&lt;/p&gt;</content><author><name>arun</name></author><category term="intel-galileo" /><category term="iot" /><summary type="html">Hello people!The following project is a basic implementation of what can later give rise to a dedicated social network for sensors. Have you ever wondered how would it be if all your inanimate things can talk on social network like Facebook, twitter? Green-house updating it’s health on it’s timeline? This project is an effort to give social life to the “things”.</summary></entry></feed>