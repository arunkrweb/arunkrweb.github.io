

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Target pose estimation and reaching using supervised learning - Arun Kumar</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Arun Kumar">
<meta property="og:title" content="Target pose estimation and reaching using supervised learning">


  <link rel="canonical" href="http://localhost:4000/articles/2018-01-target-pose-estimation-and-reaching-using-supervised-learning">
  <meta property="og:url" content="http://localhost:4000/articles/2018-01-target-pose-estimation-and-reaching-using-supervised-learning">



  <meta property="og:description" content="This is an ongoing project whose main goal is to teach manipulation tasks to the robot by observing humans perform the tasks. This observation is fed to the robot in the form of image sequences or a video. In this post I will try to explain some of the preliminary implementation of one part of the project that is estimating the target position.">



  <meta name="twitter:site" content="@ioarun">
  <meta name="twitter:title" content="Target pose estimation and reaching using supervised learning">
  <meta name="twitter:description" content="This is an ongoing project whose main goal is to teach manipulation tasks to the robot by observing humans perform the tasks. This observation is fed to the robot in the form of image sequences or a video. In this post I will try to explain some of the preliminary implementation of one part of the project that is estimating the target position.">
  <meta name="twitter:url" content="http://localhost:4000/articles/2018-01-target-pose-estimation-and-reaching-using-supervised-learning">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="http://localhost:4000/images/site-logo.png">
    
  

  



  

  















<!-- end SEO -->


<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Arun Kumar Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="http://localhost:4000/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="http://localhost:4000/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">

<script type="application/ld+json"> 
{ 
"@context": "http://schema.org",
"@type": "Person",
"name": "R. Stuart Geiger",
"email": "mailto:stuart@stuartgeiger.com",
"image": "http://stuartgeiger.com/images/oban3.jpg",
"jobTitle": "Ethnographer",
"name": "R. Stuart Geiger",
"affiliation": "University of California, Berkeley",
"alumniOf": "University of California, Berkeley",
"birthPlace": "Nacogdoches County, TX",
"gender": "male",
"honorificSuffix": "PhD",
"nationality": "United States",
"url": "http://www.stuartgeiger.com",
"sameAs" : [
"http://twitter.com/staeiou",
"http://github.com/staeiou",
"https://orcid.org/0000-0001-7215-0532"] 
} </script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="http://localhost:4000/">Arun Kumar</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/expressions/">Projects</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/year-archive/">Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/fun/">Fun</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/cv/">CV</a></li>
          
            
            <li class="masthead__menu-item"><a href="http://localhost:4000/contact/">Contact</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    

    <script type="application/ld+json">
    {
	"@context": "http://schema.org/",
	"@type": "CreativeWork",
	"author": "Arun Kumar",
        "name": "Target pose estimation and reaching using supervised learning",
	"datePublished": "",
	"description": "<p style="text-align:justify">
This is an ongoing project whose main goal is to teach manipulation tasks to the robot by observing humans perform the tasks. This observation is fed to the robot in the form of image sequences or a video. In this post I will try to explain some of the preliminary implementation of one part of the project that is estimating the target position.
</p>

"
    }
    </script>




<div id="main" role="main">
  



  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Target pose estimation and reaching using supervised learning">
    <meta itemprop="description" content="This is an ongoing project whose main goal is to teach manipulation tasks to the robot by observing humans perform the tasks. This observation is fed to the robot in the form of image sequences or a video. In this post I will try to explain some of the preliminary implementation of one part of the project that is estimating the target position.">
    
    
    


    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Target pose estimation and reaching using supervised learning
</h1>
          
        
        </header>
      

      <section class="page__content" itemprop="text">
        <p style="text-align:justify">
This is an ongoing project whose main goal is to teach manipulation tasks to the robot by observing humans perform the tasks. This observation is fed to the robot in the form of image sequences or a video. In this post I will try to explain some of the preliminary implementation of one part of the project that is estimating the target position.
</p>

<!-- _includes/image.html -->
<center>
<div class="image-wrapper">
    
    <a href="http://arunkrweb.github.io/images/2018/imitation-learning/pose-estimation.gif" title="Pose Estimation">
    
        <img align="middle" src="http://localhost:4000//images/2018/imitation-learning/pose-estimation.gif" alt="Pose Estimation" style="border: 2px solid black;" />
    
    </a>
    
    
        <p align="center" class="image-caption" style="font-size:14px;">Fig 1. CNN predicts the target pose for reaching task</p>
    
</div>
</center>

<h3>Environment</h3>
<p style="text-align:justify">
 The simulation is set up in the Gazebo simulator. The robot being used is 7DOF Sawyer robot with an electronic gripper attached to the end-effector. A camera is placed on top of the workspace to capture images to be fed to the network. A table is also spawned in the world on top of which a blue cube randomly spawns.</p>

<p style="text-align:justify"> <b>Task</b> <br />
 To predict the blue cube's position on the table and reach to that position.</p>

<h3>Architecture</h3>

<!-- _includes/image.html -->
<center>
<div class="image-wrapper">
    
    <a href="http://arunkrweb.github.io/images/2018/imitation-learning/spatial-softmax-using-cnn.jpg" title="Architecture">
    
        <img align="middle" src="http://localhost:4000//images/2018/imitation-learning/spatial-softmax-using-cnn.jpg" alt="Architecture" style="border: 2px solid black;" />
    
    </a>
    
    
        <p align="center" class="image-caption" style="font-size:14px;">Fig 2. CNN architecture to predict cube position</p>
    
</div>
</center>

<p style="text-align:justify">

<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

The network consists of 3 convolutional layers which learn a bank of filters. These filters form a hierarchy of local image features. The convolution operation is followed by a rectifying nonlinearity of the form \(a_{cij}=\max(0, z_{ij})\) for each channel <i>c</i> and each pixel coordinate \((i,j)\). The third convolutional layer contains 32 response maps. These response maps are passed through a spatial softmax function of the form \(s_{cij} = {e^{a_{cij}}}/{\sum_{i^{'} j^{'}}{e^{a_{ci^{'}j^{'}}}}}\). Each output channel of the softmax is a probability distribution over the location of a feature in the image. To convert from this distribution to a coordinate representation \((f_{cx}, f_{cy} )\), the network calculates the expected image position of each feature, yielding a 2D coordinate for each channel: \(f_{cx} = \sum_{ij}{s_{cij}x_{ij}}\) and \(f_{cy} = \sum_{ij}{s_{cij}y_{ij}}\) where \((x_{ij}, y_{ij})\) is the image-space position of the point \((i, j)\) in the response map. <br />

The spatial softmax and the expected position computation serve to convert pixel-wise
representations in the convolutional layers to spatial coordinate representations, which can be manipulated by the fully connected layers into 3D positions. 
</p>

<h3>Input image during test</h3>

<p style="text-align:justify">
	For demonstration purpose, I randomly sampled two images from the test data set. As can be seen in the following figure, cube is spawned on two different positions on the table.
</p>

<!-- _includes/image.html -->
<center>
<div class="image-wrapper">
    
    <a href="http://arunkrweb.github.io/images/2018/imitation-learning/input_images.jpg" title="input images">
    
        <img align="middle" src="http://localhost:4000//images/2018/imitation-learning/input_images.jpg" alt="input images" style="border: 2px solid black;" />
    
    </a>
    
    
        <p align="center" class="image-caption" style="font-size:14px;">Fig 3. Input to the CNN</p>
    
</div>
</center>

<h3>Spatial softmax for the two inputs looks as follows</h3>
<ul>
	<li> Image 1 (cube on left)</li>

	<!-- _includes/image.html -->
<center>
<div class="image-wrapper">
    
    <a href="http://arunkrweb.github.io/images/2018/imitation-learning/150_softmax.jpg" title="input images">
    
        <img align="middle" src="http://localhost:4000//images/2018/imitation-learning/150_softmax.jpg" alt="input images" style="border: 2px solid black;" />
    
    </a>
    
    
        <p align="center" class="image-caption" style="font-size:14px;">Fig 4. Spatial softmax for image 1</p>
    
</div>
</center>


	<li> Image 2 (cube on right)</li>
	<!-- _includes/image.html -->
<center>
<div class="image-wrapper">
    
    <a href="http://arunkrweb.github.io/images/2018/imitation-learning/156_softmax.jpg" title="input images">
    
        <img align="middle" src="http://localhost:4000//images/2018/imitation-learning/156_softmax.jpg" alt="input images" style="border: 2px solid black;" />
    
    </a>
    
    
        <p align="center" class="image-caption" style="font-size:14px;">Fig 5. Spatial softmax for image 2</p>
    
</div>
</center>

</ul>

<p style="text-align:justify;">
	What you see in Fig 4. and Fig 5. are the results of softmax operation on the activation maps of third convolutional layers. This operation results in a probability distribution over features in each activation map. There are 32 activation maps in total because of the 32 filters used in the convolution operation. The softmax operation points to the area of maximum activation in each activation map i.e, the most important feature. The next step, as discussed before, is to calculate the expected 2D position of each feature.
</p>

<h3> Results </h3>

<p style="text-align:justify;">
 The results for image 1 are as follows: <br />
 predicted position : [ 0.72932047, -0.3483142 ] <br />
 true position : [0.72, -0.35]<br />
 test loss : 4.485624e-05
</p>
<p style="text-align: justify;">
 As can be seen from the results, the network predicts the cube's position pretty well. This acts as the target position for the robot's end effector. The IK solver calculates the joint angles to be reached and the robot's controller tries to reach this configuration. 
</p>

<h3>Reference</h3>
<p style="text-align:justify; font-size: 12px;">
Levine, Sergey, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. "End-to-end training of deep visuomotor policies." The Journal of Machine Learning Research 17, no. 1 (2016): 1334-1373.
</p>


        
      </section>

      <footer class="page__meta">
        
        




  






  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="http://localhost:4000/categories/#articles" class="page__taxonomy-item" rel="tag">articles</a>
    
    </span>
  </p>


      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=http://localhost:4000/articles/2018-01-target-pose-estimation-and-reaching-using-supervised-learning" class="btn btn--twitter" title="Share on Twitter"><i class="fa fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/articles/2018-01-target-pose-estimation-and-reaching-using-supervised-learning" class="btn btn--facebook" title="Share on Facebook"><i class="fa fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://plus.google.com/share?url=http://localhost:4000/articles/2018-01-target-pose-estimation-and-reaching-using-supervised-learning" class="btn btn--google-plus" title="Share on Google Plus"><i class="fa fa-fw fa-google-plus" aria-hidden="true"></i><span> Google+</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http://localhost:4000/articles/2018-01-target-pose-estimation-and-reaching-using-supervised-learning" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fa fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>

      


  <nav class="pagination">
    
      <a href="#" class="pagination--pager disabled">Previous</a>
    
    
      <a href="http://localhost:4000/articles/2018-02-bipedal-walking-robot-using-rl" class="pagination--pager" title="Bipedal Walking Robot using Reinforcement Learning
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
<!--    -->
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
      <li><a href="https://twitter.com/ioarun"><i class="fa fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
    
    
    
      <li><a href="http://github.com/ioarun"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
   <!--  <li><a href="http://localhost:4000/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li> -->
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2018 Arun Kumar. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>
      </footer>
    </div>

    <script src="http://localhost:4000/assets/js/main.min.js"></script>





  </body>
</html>

